# %%
import cv2
from bovw import BOVW
import pandas
from typing import *
from PIL import Image
import matplotlib.pyplot as plt 
from main import Dataset, run_final_pipeline, run_BOVW_experiment
from IPython.display import clear_output

# %% [markdown]
# Loading the data

# %%
print("Loading Train Dataset...")
data_train = Dataset(ImageFolder="../places_reduced/train")

print("Loading Test Dataset...")
data_test = Dataset(ImageFolder="../places_reduced/val")

# %%


# %% [markdown]
# #### Run the entire suit of tests and save them to final_experiment_results.csv

# %%
# run_final_pipeline(data_train, data_test) # Uncomment for delivery

# %% [markdown]
# #### Loading the results csv from previous executions

# %%
results = pandas.read_csv("final_experiment_results.csv")

# %% [markdown]
# #### Results of the first experiment

# %%
print(results[results["Experiment"] == "Classifiers"])

# %% [markdown]
# #### Running a given experiment and keeping the labels for qualitative results

# %%
classifiers = ["log_reg", "svm_linear", "svm_rbf", "rf"]
experiment_one_acc = {}
experiment_one_data = {}

for clf in classifiers:
    print("Running classifier", clf)
    bovw = BOVW(detector_type="SIFT", codebook_size=128)
    experiment_one_acc[clf], experiment_one_data[clf] = run_BOVW_experiment(bovw,data_train,data_test, clf_name=clf)

clear_output()
print("Experiment 1 succesfull")

# %% [markdown]
# ### Experiment: Effect of Codebook Size (k) on BoVW Performance
# 
# 

# %%
import matplotlib.pyplot as plt
import seaborn as sns

df_k = results[results["Experiment"] == "Codebook Size"].copy()


display(df_k[["Value", "CV_Accuracy", "Test_Accuracy"]])

plt.figure(figsize=(10,5))
sns.lineplot(x=df_k["Value"], y=df_k["CV_Accuracy"], marker="o", label="CV Accuracy")
sns.lineplot(x=df_k["Value"], y=df_k["Test_Accuracy"], marker="o", label="Test Accuracy")
plt.title("Codebook Size (k) vs Accuracy — BoVW")
plt.xlabel("Codebook Size (k)")
plt.ylabel("Accuracy")
plt.grid(True)
plt.legend()
plt.show()

best = df_k.loc[df_k["Test_Accuracy"].idxmax()]
print("\nBest performing codebook size:")
display(best)

print(f"\nConclusion: k={best['Value']} gives best performance "
        f"(Test Accuracy = {best['Test_Accuracy']:.4f}).")


# %% [markdown]
# In this experiment, we evaluate how changing the **codebook size** in the Bag-of-Visual-Words (BoVW) pipeline affects classification performance.
# 
# We observed:
# - Small codebooks (k = 32–64) didn’t perform very well. They don’t capture enough variety in the image features.
# - Medium sizes (k = 128) gave the best results. This is where the model captures enough detail without becoming too sparse.
# - Large sizes (k = 256–512) didn’t really improve accuracy. The histograms become too sparse due to overfitting and the classifier benefits less.
# 

# %% [markdown]
# ### Qualitative Example: How Codebook Size Affects Predictions
# 
# To better understand the impact of codebook size on image classification, we compare predictions for a single test image across different values of **k**.
# 
# For each codebook size:
# - We display the predicted class.
# - We highlight correct predictions in **green** and incorrect ones in **red**.
# - This helps visually analyze how increasing k affects model behavior on individual samples.

# %%
def plot_imgs_at_idx(target_index,data_test,k_,data):
    classes = [
'shopping and dining', 'sports and leisure', 'water_ice_snow', 'mountains_hills_desert_sky', 'workplace', 'home or hotel', 'forest_field_jungle', 'industrial and construction', 'houses_cabins_gardens_farms', 'commercial buildings', 'sports_fields'

    ]

    fig, (ax_img, ax_text) = plt.subplots(1, 2, figsize=(8, 4), gridspec_kw={'width_ratios': [1, 1]})
    # --- Image Plot ---
    ax_img.imshow(data_test[target_index][0], cmap='gray')
    ax_img.set_title(f"Image at Index {target_index}", fontsize=12)
    ax_img.axis('off')

    # --- Text Plot ---
    ax_text.axis('off') # Hide axes for the text area
    ax_text.set_title("Mismatch Predictions", fontsize=12)

    text_y = 0.9
    line_height = 0.15

    # Iterate through the indices where the secondary prediction was FALSE
    for k in k_:
        predicted_class = classes[data[k][0][target_index]]
        
        # Format the text string
        text_line = f"K: {k}\nPrediction: {predicted_class}"
        if data[k][0][target_index] == data[k][1][target_index]:
            # Add the text in green
            ax_text.text(0.1, text_y, text_line, color='green', fontsize=11, verticalalignment='top')
        else:
            ax_text.text(0.1, text_y, text_line, color='red', fontsize=11, verticalalignment='top')
        text_y -= line_height


    plt.tight_layout()
    plt.show()


codebook_data_acc = {}
codebook_data = {}
codebook_sizes =  [32,64,128,256,512]
for codebook in codebook_sizes:
        bovw = BOVW(detector_type="SIFT", codebook_size=codebook)
        codebook_data_acc[codebook], codebook_data[codebook] = run_BOVW_experiment(bovw,data_train,data_test,)


clear_output()
print("Experiment codebook succesfull")

index = 24

plot_imgs_at_idx(index,data_test,codebook_sizes,codebook_data)



# %% [markdown]
# ### Fisher Vector Performance Across GMM Components (K)
# 
# Here we examine how Fisher Vector performance changes with the number of **Gaussian Mixture Model (GMM)** components.
# 
# Key observations:
# - Increasing K produces better, higher-dimensional Fisher Vectors.
# - Both **cross-validation accuracy** and **test accuracy** are analyzed.
# - FV generally performs better than BoVW and scales better with K.
# 
# This plot helps identify the point where adding more Gaussian components provides diminishing returns.
# 
# ### BoVW vs Fisher Vectors — Accuracy Comparison
# 
# This visualization directly compares **BoVW test accuracy** with **Fisher Vector test accuracy** for matching values of K.
# 
# - BoVW uses K as the **number of visual words**.
# - Fisher Vectors use K as the **number of GMM components**.
# - A grouped bar chart shows which representation performs better at each K.
# 
# This combined comparison clearly highlights the performance gap and shows how Fisher Vectors outperform BoVW across nearly all values of k.
# 

# %%
sns.set(style="whitegrid", font_scale=1.2)

# Load FV results
df_fv = pd.read_csv("fisher_vector_results.csv")
print("Loaded Fisher Vector results:")
display(df_fv)


# Load BoVW (Codebook Size) results for comparison
df_k = results[results["Experiment"] == "Codebook Size"].copy()


# Only test accuracy is needed for comparison
bovw_best = df_k.loc[df_k["Test_Accuracy"].idxmax()]
print("\nBest BoVW configuration:")
display(bovw_best)


# 1) Fisher Vector Trend Plot

plt.figure(figsize=(10,5))
plt.plot(df_fv["K"], df_fv["test_acc"], marker="o", linewidth=3, label="FV Test Accuracy")
plt.plot(df_fv["K"], df_fv["cv_mean_acc"], marker="s", linestyle="--", linewidth=2, label="FV CV Accuracy")

plt.title("Fisher Vector Performance vs Number of GMM Components (K)")
plt.xlabel("Number of Gaussian Components (K)")
plt.ylabel("Accuracy")
plt.xticks(df_fv["K"])
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

### Fisher Vector vs BoVW: Grouped Bar Plot (Accuracy vs K)


# Load FV and BoVW results

df_fv = pd.read_csv("fisher_vector_results.csv")
df_k = results[results["Experiment"] == "Codebook Size"].copy()


# Standardize column names

df_fv_small = df_fv[["K", "test_acc"]].rename(columns={"test_acc": "FV_Accuracy"})
df_bovw_small = df_k[["Value", "Test_Accuracy"]].rename(
    columns={"Value": "K", "Test_Accuracy": "BoVW_Accuracy"}
)


# Fix both Ks as integers

df_fv_small["K"] = pd.to_numeric(df_fv_small["K"], errors="coerce").astype("Int64")
df_bovw_small["K"] = pd.to_numeric(df_bovw_small["K"], errors="coerce").astype("Int64")


# Merge on K, outer join keeps all existing K values

df_merge = pd.merge(df_bovw_small, df_fv_small, on="K", how="outer").sort_values("K")

print("Merged Accuracy Table:")
display(df_merge)


# Plot: grouped bar chart

plt.figure(figsize=(12,6))

bar_width = 0.35
x = range(len(df_merge))

plt.bar(
    [p - bar_width/2 for p in x], 
    df_merge["BoVW_Accuracy"], 
    width=bar_width, 
    label="BoVW", 
    color="#1f77b4"
)

plt.bar(
    [p + bar_width/2 for p in x], 
    df_merge["FV_Accuracy"], 
    width=bar_width, 
    label="Fisher Vector", 
    color="#ff7f0e"
)

plt.xticks(x, df_merge["K"])
plt.xlabel("K (Codebook Size / GMM Components)")
plt.ylabel("Accuracy")
plt.title("BoVW vs Fisher Vector — Accuracy vs K")
plt.ylim(0, 1)
plt.legend()
plt.grid(axis='y', linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()



